{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: digit recognizer\n",
    "---\n",
    "\n",
    " Some say that the \"Hello World!\" of machine learning is building a classifier that recognizes 28x28 digits using the MNIST hand-written digit database. Sure, there are plenty of copy-and-paste tutorials out there, but here is my implementation where I have made an effort to write and understand every line within my implementation. \n",
    " \n",
    "This is a learning exercise for me, so there are plenty of notes inside to keep track of what I am doing and why. Enjoy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implementation details\n",
    "- Implemented on Windows 10 (TF session setup may need to be tweaked for Linux run, see Python 3 'os' docs for details)\n",
    "- Graphics card: GeForce GTX 1050 Ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup successful!\n"
     ]
    }
   ],
   "source": [
    "# Project setup\n",
    "try:\n",
    "    import os as os\n",
    "    import sys as sys\n",
    "    import keras as k\n",
    "    from keras.backend import tensorflow_backend as tb\n",
    "    from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "    from keras.models import Sequential\n",
    "    from keras.utils.np_utils import to_categorical # for one-hot-encoding\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import tensorflow as tf\n",
    "    print('Setup successful!')\n",
    "except Exception as e:\n",
    "    print(\"Setup failed:\\n\\t\" + str(e))\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF session setup\n",
    "random_seed = 2\n",
    "np.random.seed(random_seed)\n",
    "if tb._SESSION is None:\n",
    "    if not os.environ.get('OMP_NUM_THREADS'):\n",
    "#         print(\"os.environ.get('OMP_NUM_THREADS') not found.\")\n",
    "        config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    else:\n",
    "#         print(\"TRUE OMP NUM THREADS, RESULT =\", os.environ.get('OMP_NUM_THREADS'))\n",
    "        num_thread = int(os.environ.get('OMP_NUM_THREADS'))\n",
    "        config = tf.ConfigProto(intra_op_parallelism_threads=num_thread, allow_soft_placement=True)\n",
    "    config.gpu_options.allow_growth=True\n",
    "    _SESSION = tf.Session(config=config)\n",
    "session = _SESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\", dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df.label.astype(np.int64).copy()\n",
    "features = df.drop(labels=[\"label\"], axis=1).copy() / 255.0\n",
    "# print(\"Number of columns in 'features':\", len(features.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a histogram of label distribution\n",
    "# Uncomment this later\n",
    "# sns.countplot(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# features.values.reshape(-1, 28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode labels\n",
    "# Reshape input pixel data from 1x784 to 28x28(x1)\n",
    "features = features.values.reshape(-1,28,28,1)\n",
    "\n",
    "\n",
    "# labels = labels.values.reshape(-1,28,28,1)\n",
    "# Onehot encode labels\n",
    "labels = to_categorical(labels, num_classes = 10)\n",
    "# Flip it from a row vector to a column vector\n",
    "labels = labels[:,0:1]\n",
    "# sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.1, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a digit\n",
    "# plt.imshow(features[0][:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras models/layers details\n",
    "\n",
    "- filters: dimensionality of output space\n",
    "- kernel size: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=28,kernel_size=(1,1),\n",
    "                padding = 'Same', activation='relu',\n",
    "                input_shape=(28,28,1)))\n",
    "# model.add(Conv2D(filters=100,kernel_size=(5,5),\n",
    "#                 padding = 'Same', activation='relu',\n",
    "#                 input_shape=(28,28,1)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare optimizer and compile\n",
    "# lr=learning rate, \n",
    "# rho=\"Gradient M.A. decay factor\",\n",
    "# epsilon=fuzz factor,\n",
    "# decay = delta lr per update,\n",
    "optimizer = k.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# model.fit variables\n",
    "epochs = 2\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "37800/37800 [==============================] - 7s 193us/step - loss: 0.0447 - acc: 0.9863\n",
      "\n",
      "4200/4200 [==============================] - 1s 152us/step\n",
      "Accuracy: 0.9871428571428571 | Loss: 0.03991178985876364\n"
     ]
    }
   ],
   "source": [
    "#     history = model.fit(features_train, labels_train, batch_size = batch_size, epochs = epochs, \n",
    "#              validation_data = (features_test, labels_test), verbose = 2)\n",
    "\n",
    "\n",
    "model.fit(features_train, labels_train, epochs=epochs, batch_size=batch_size)\n",
    "print()\n",
    "loss_and_metrics = model.evaluate(features_test, labels_test) # WHAT IS THE DEFAULT BATCH SIZE?\n",
    "\n",
    "#     print(\"\\n\", loss_and_metrics)\n",
    "print(\"Accuracy: \",loss_and_metrics[1], \" | Loss: \", loss_and_metrics[0], sep=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
